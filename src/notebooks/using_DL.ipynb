{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Input, Dense, Concatenate, BatchNormalization, Dropout\n",
    "from tensorflow.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NO_WORDS = 200000\n",
    "MAX_SEQUENCE_LEN = 30\n",
    "VOCAB_SIZE = 0  # will be updated after tokenizing\n",
    "EMBEDDINGS_DIM = 300\n",
    "GLOVE_EMBEDDINGS_FILEPATH = ''\n",
    "MODEL_FILEPATH = 'dl_model.h5'\n",
    "MODEL_ARCHITECTURE_FILEPATH = 'model_architecture.png'\n",
    "MODEL_CHECKPOINT_FILEPATH = 'lstm_model_three_.{epoch:02d}-{val_loss:.6f}.h5'\n",
    "\n",
    "VALIDATION_SPLIT = 0.3\n",
    "RATE_DROP_LSTM = 0.17\n",
    "RATE_DROP_DENSE = 0.25\n",
    "NUMBER_DENSE_UNITS = 64\n",
    "ACTIVATION_FUNCTION = 'relu'\n",
    "LEARNING_RATE_REDUCTION_FACTOR = 0.9\n",
    "MIN_EPCOHS_NO_IMPROVEMENT_BEFORE_SAVING_CHECKPOINT = 0.8\n",
    "MIN_EPOCHS_NO_IMPROVEMENT_BEFORE_REDUCING_LR = 0.2\n",
    "MINIMUM_LR = 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings_in_dict():\n",
    "    embeddings = {}\n",
    "    file = open(GLOVE_EMBEDDINGS_FILEPATH)\n",
    "    for line in file:\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "        embedding = line[1:]\n",
    "        embeddings[word] = embedding\n",
    "    \n",
    "    file.close()\n",
    "    return embeddings\n",
    "\n",
    "def filter_embeddings(tokenizer, loaded_embeddings: dict):\n",
    "    \"\"\"For words present in our vocabulary, we're using embedding from loaded embeddings if the word is present there else using zeros.\"\"\"\n",
    "    filtered_embeddings = np.array((VOCAB_SIZE+1, EMBEDDINGS_DIM))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        word_embedding = loaded_embeddings.get(word)\n",
    "        if word_embedding is not None:\n",
    "            filtered_embeddings[i] = word_embedding\n",
    "    return filtered_embeddings\n",
    "\n",
    "def create_length_features(questions_1: list, questions_2: list):\n",
    "    \"\"\"The inputs needs to be a list of lists. We create three features i.e. length of unique words in q1 and same for q2 and len of common words.\"\"\"\n",
    "    length_features = [[len(set(question1)), len(set(question2)), len(set(question1).intersection(set(question2)))] for question1, question2 in zip(questions_1, questions_2)]\n",
    "    return np.array(length_features, dtype = 'float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "questions = list(df['question1']) + list(df['question2'])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NO_WORDS, filters='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(questions)\n",
    "VOCAB_SIZE = len(tokenizer.word_counts) # unique words\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(df['question1'])\n",
    "sequences_2 = tokenizer.texts_to_sequences(df['question2'])\n",
    "\n",
    "sequences_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LEN)\n",
    "sequences_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_col_categorized = to_categorical(df['is_duplicate'], num_classes=2)\n",
    "\n",
    "# creating embeddings\n",
    "embeddings = read_embeddings_in_dict()\n",
    "print(f\"Embeddings vocabulary size: {len(embeddings)}. Our data vocabulary size: {VOCAB_SIZE}\")\n",
    "embeddings = filter_embeddings(tokenizer, embeddings)\n",
    "print(f\"Filtered embeddings vocabulary size: {len(embeddings)}.\")\n",
    "\n",
    "# develping length features\n",
    "length_feats = create_length_features(sequences_1, sequences_2)\n",
    "\n",
    "# data splittion\n",
    "sequences_1train, sequences_1test, sequences_2train, sequences_2test, length_feats_train, length_feats_test, target_train, target_test = train_test_split(\n",
    "    sequences_1,\n",
    "    sequences_2,\n",
    "    length_feats,\n",
    "    target_col_categorized,\n",
    "    test_size=0.3,\n",
    "    random_state=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN Architecture Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NN Architecture Setup\n",
    "\n",
    "# creating embeddings layer\n",
    "embedding_layer = Embedding(input_dim=VOCAB_SIZE+1, output_dim=EMBEDDINGS_DIM, input_length=MAX_SEQUENCE_LEN, weights=[embeddings], trainable=False)\n",
    "# creating lstm layer\n",
    "lstm_layer = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))     # recurrent_dropout causes dropout in the internal gate neurons because the standard dropout only words for input and output gate neurons\n",
    "\n",
    "# creating lstm_sequences for the 1st sentence\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32')\n",
    "embeddings_sequence_1 = embedding_layer(sequence_1_input)\n",
    "lstm_for_sequence_1 = lstm_layer(embeddings_sequence_1)\n",
    "\n",
    "# creating lstm_sequences for the 2nd sentence\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32')\n",
    "embeddings_sequence_2 = embedding_layer(sequence_2_input)\n",
    "lstm_for_sequence_2 = lstm_layer(embeddings_sequence_2)\n",
    "\n",
    "# creating leak input\n",
    "length_feats_input = Input(shape=(length_feats_train.shape[1],))\n",
    "length_feats_dense = Dense(int(NUMBER_DENSE_UNITS/2), activation=ACTIVATION_FUNCTION)(length_feats_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# concatenating output of lstm and dense layer (processing both texts and length features properly)\n",
    "merged_layers = Concatenate([lstm_for_sequence_1, lstm_for_sequence_2, length_feats_dense])\n",
    "merged_layers = BatchNormalization()(merged_layers)\n",
    "merged_layers = Dropout(RATE_DROP_DENSE)(merged_layers)\n",
    "merged_layers = Dense(NUMBER_DENSE_UNITS, activation=ACTIVATION_FUNCTION)(merged_layers)\n",
    "merged_layers = BatchNormalization()(merged_layers)\n",
    "merged_layers = Dropout(RATE_DROP_DENSE)(merged_layers)\n",
    "output_layer = Dense(2, activation='sigmoid')(merged_layers)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input, length_feats_input], outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying model architecture\n",
    "plot_model(model, MODEL_ARCHITECTURE_FILEPATH)\n",
    "Image.open(MODEL_ARCHITECTURE_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopper and Checkpoint saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "earlystopper = EarlyStopping(patience=MIN_EPCOHS_NO_IMPROVEMENT_BEFORE_SAVING_CHECKPOINT, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath=MODEL_CHECKPOINT_FILEPATH, save_best_only=True, save_weights_only=True, verbose=1)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', patience=MIN_EPOCHS_NO_IMPROVEMENT_BEFORE_REDUCING_LR, factor=LEARNING_RATE_REDUCTION_FACTOR, min_lr=MINIMUM_LR, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([sequences_1train, sequences_2train, length_feats_train], target_train, validation_data=([sequences_1test, sequences_2test, length_feats_test], target_test), verbose=1, epochs=200, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "print(f\"Training loss: {train_loss} \\\n",
    "      \\nTest loss: {val_loss} \\\n",
    "      \\nTrain accuracy: {train_acc} \\\n",
    "      \\nTest accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, color='red', label='Train loss')\n",
    "plt.plot(val_loss, color='blue', label='Test loss')\n",
    "plt.title(\"Loss visualization\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig('Loss-visualization.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_acc, color='red', label='Train loss')\n",
    "plt.plot(val_acc, color='blue', label='Test loss')\n",
    "plt.title(\"Accuracy visualization\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('Accuracy-visualization.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
